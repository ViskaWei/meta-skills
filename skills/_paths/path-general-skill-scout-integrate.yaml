id: path-general-skill-scout-integrate
name: "Skill Scout & Integrate — Discover External Skills and Adapt to 3-Layer System"
domain: general
version: 1
outcome: "Need statement → Scout external ecosystem → Evaluate quality → Adapt to 3-layer architecture → Register → Deploy → Knowledge capture"
source: "Built from research on skills.sh, awesome-agent-skills, Anthropic official skills, Skill Curator scoring"
execution_mode: team
parallel_groups: []

description: >
  Proactively explores the external skill ecosystem (GitHub awesome lists, npx skills,
  Anthropic official repo, community repos) to find high-quality skills that fill gaps
  in our system. Evaluates each candidate on 5 quality dimensions, adapts to our 3-layer
  architecture (as L2 tools or policies), registers, deploys, and captures knowledge.
  This is NOT `find-skills` (which just runs `npx skills find`) — this is a full
  scout → evaluate → integrate pipeline that produces registered, deployed capabilities.

triggers:
  - "capture skill"
  - "scout skills"
  - "探索技能"
  - "发现技能"
  - "skill scout"

steps:
  # === Phase 1: Define need (discover) ===
  - id: define-need
    stage: discover
    capabilities_needed: [cap-intake-brief]
    output_type: brief
    description: >
      Clarify what capability gaps we're trying to fill:
      1. If user specifies a domain/topic → use that as search query
      2. If user says "全面探索" → scan our current system for coverage gaps:
         - Read skills-registry.yaml L0 triggers → identify domains with thin coverage
         - Read _tools/ families → identify families with few tools
         - Read capability-index.yaml → identify verbs with few cap-* entries
      3. Produce a brief listing:
         - Target domains/capabilities to scout for
         - What we already have (to avoid duplicates)
         - Quality bar: minimum G2 (domain-portable) for integration

  # === Phase 2: Scout external ecosystem (discover) ===
  - id: scout-ecosystem
    stage: discover
    capabilities_needed: [cap-extract-brief, cap-map-context]
    output_type: source-pack
    gate_requires: [brief]
    description: >
      Search multiple sources for candidate skills. For each source, extract
      skill name, description, source URL, and relevance to our needs.

      **Source priority order:**

      1. **Anthropic official** (https://github.com/anthropics/skills/tree/main/skills)
         - Highest trust, official quality
         - Check: do we already have these installed?

      2. **Awesome curated lists** (web search for latest)
         - https://github.com/VoltAgent/awesome-agent-skills (380+ skills)
         - https://github.com/travisvn/awesome-claude-skills
         - https://github.com/ComposioHQ/awesome-claude-skills
         - https://github.com/hesreallyhim/awesome-claude-code
         - Filter: only skills from known teams (Vercel, Stripe, Google, etc.)

      3. **Skills.sh directory** (search by keyword)
         - Run: `npx skills find "<domain keyword>"` for each target domain
         - Parse output for skill names and descriptions

      4. **GitHub search** (for niche domains)
         - Search: `SKILL.md <domain> language:Markdown`
         - Filter: repos with >50 stars, updated in last 6 months

      **For each candidate, record:**
      - name, description, source URL
      - author/org (official team vs community)
      - stars/popularity signal
      - last updated date
      - estimated token count (SKILL.md size)

      **Dedup against existing:**
      - Check ~/.claude/skills/ for already-installed skills
      - Check _tools/ for existing coverage of same capability
      - Skip exact duplicates, flag near-duplicates for comparison

      Produce a candidate list of 10-30 skills ranked by relevance.

  # === Phase 3: Evaluate quality (decide) ===
  - id: evaluate-quality
    stage: decide
    capabilities_needed: [cap-compare-option-matrix]
    output_type: option-matrix
    gate_requires: [source-pack]
    description: >
      For each candidate skill (top 10-15 by relevance), fetch and read
      the actual SKILL.md content, then evaluate on 5 dimensions:

      **Quality Scoring Matrix (each 1-10):**

      | Dimension | Weight | Criteria |
      |-----------|--------|----------|
      | Practicality | 25% | Solves a real, recurring need in our workflow |
      | Clarity | 20% | Well-structured SKILL.md, clear triggers, good examples |
      | Automation | 20% | Includes scripts/tools, not just prose instructions |
      | Depth | 20% | Comprehensive coverage, handles edge cases |
      | Fit | 15% | Compatible with our 3-layer architecture, no conflicts |

      **Score thresholds:**
      - S-rank (≥8.5): Auto-include, high priority integration
      - A-rank (≥7.0): Include if fills a gap
      - B-rank (≥5.5): Consider if no better alternative
      - C-rank (<5.5): Skip

      **Additional checks:**
      - Token budget: SKILL.md body < 500 lines (per Anthropic best practices)
      - No time-sensitive content
      - Consistent terminology
      - Progressive disclosure (references, not inline bloat)
      - Description is specific (not "helps with documents")

      **PAUSE: Present evaluation matrix to user.**
      Show: skill name, source, score, rank, recommended action (integrate/skip/compare).
      User confirms which skills to integrate before proceeding.

  # === Phase 4: Adapt to 3-layer architecture (build) ===
  - id: adapt-skills
    stage: build
    capabilities_needed: [cap-build-implementation]
    output_type: patched-artifact
    gate_requires: [option-matrix]
    description: >
      For each approved skill, adapt it to our 3-layer architecture:

      **Step 4a: Determine placement**
      | External skill type | Our placement | Naming |
      |---------------------|---------------|--------|
      | Domain-specific tool | `_tools/<family>/<name>.md` | keep original name (kebab-case) |
      | Quality/verification | `_policies/rule-<scope>-<intent>.yaml` | follow rule naming |
      | Multi-step workflow | `_paths/path-<domain>-<outcome>.yaml` | follow path naming |
      | General reference | `_tools/<family>/<name>.md` | keep as reference doc |

      **Step 4b: Content adaptation**
      For each skill file:
      1. Keep the core instructions intact (don't rewrite working content)
      2. Strip YAML frontmatter (our _tools don't use it, only top-level SKILL.md does)
      3. Add a source attribution header:
         ```
         # <Skill Name>
         > Source: <GitHub URL> | Adapted: <date> | Original author: <author>
         ```
      4. Check for dependency conflicts (pip/npm packages we don't have)
      5. Verify file references are valid (no broken links to files we didn't copy)
      6. If skill has sub-files (reference/, scripts/), copy them into the same _tools/ family

      **Step 4c: Handle already-installed duplicates**
      If the skill is already in ~/.claude/skills/ as a flat install:
      - Compare our adapted version vs the flat install
      - If our version is richer (3-layer integrated), remove the flat install
      - If flat install has unique content, merge into our version

      **Do NOT:**
      - Rewrite working instructions (adapt, don't rebuild)
      - Break existing _tools/ family organization
      - Install skills that duplicate existing L2 capabilities
      - Use verbs/objects not in our controlled vocabulary for cap-* names

  # === Phase 5: Register and deploy (build + verify) ===
  - id: register-deploy
    stage: verify
    capabilities_needed: [cap-decide-quality-gate]
    output_type: gate-verdict
    gate_requires: [patched-artifact]
    description: >
      Register and deploy each adapted skill:

      **Registration (if creating new cap-* entries):**
      1. Add objects to `_resolver/objects.yaml` (if new objects needed)
      2. Add entries to `_resolver/capability-index.yaml`
      3. Add entries to `skills-registry.yaml`
      4. Update count references in CLAUDE.md and docs/3-layer-summary.md

      **Registration (if just adding _tools/ files):**
      1. No cap-* registration needed (domain tools don't require it)
      2. Verify file is in correct _tools/<family>/ directory
      3. Verify kebab-case naming

      **Deploy:**
      1. `bash tools/setup.sh` → all OK, no FAIL
      2. Verify new files appear in `~/.claude/skills/_tools/`
      3. If any flat-install duplicates were marked for removal, verify they're gone

      **Verify:**
      - New tool files exist and are readable
      - No YAML parse errors in registry files
      - capability-index counts match actual files
      - `tools/validate_contracts.sh` passes (if applicable)

  # === Phase 6: Knowledge capture (knowledge) ===
  - id: capture-scout-result
    stage: knowledge
    capabilities_needed: [cap-capture-card]
    output_type: knowledge-card
    gate_requires: [gate-verdict]
    description: >
      Create a scout report knowledge card:

      ```markdown
      # Skill Scout Report — <date>

      ## Scout Summary
      - Sources searched: <list>
      - Candidates evaluated: N
      - Skills integrated: M
      - Skills skipped: K (with reasons)

      ## Integrated Skills

      | Skill | Source | Score | Placement | Family |
      |-------|--------|-------|-----------|--------|
      | <name> | <url> | <rank> | _tools/<family>/ | <family> |

      ## Skipped Skills (notable)

      | Skill | Reason |
      |-------|--------|
      | <name> | duplicate of <existing> / low quality / incompatible |

      ## Coverage Gaps Remaining
      - <domain>: still needs <capability>

      ## Recommendations for Next Scout
      - <what to look for next time>
      ```

      Save to: `knowledge/skill-scout/scout-<date>.md`

branches:
  - from: register-deploy
    condition: "FAIL AND iterations < max_iterations"
    goto: adapt-skills
    note: "Fix registration/deployment issues"
  - from: register-deploy
    condition: "PASS"
    goto: capture-scout-result
  - from: evaluate-quality
    condition: "ALL candidates < B-rank"
    goto: capture-scout-result
    note: "No quality candidates found — still capture the scout report"

applicable_policies:
  required:
    - rule-quality-deliverable-minimum
    - rule-completion-guard
  recommended:
    - rule-capability-gap-detection
    - rule-improve-verify-result

stop_rules:
  max_iterations: 2
  deploy_must_pass: "bash tools/setup.sh shows all OK"
  user_approval: "Must confirm candidates before integration (Phase 3 PAUSE)"
completion_guard:
  enabled: true
  required_evidence: [gate-verdict, knowledge-card]
  completion_promise: "ALL_STEPS_COMPLETE"
